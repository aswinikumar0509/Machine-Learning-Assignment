{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9580cec",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "Ans:\n",
    "Support Vector Machines (SVM) is a supervised learning algorithm used for classification and regression analysis. The underlying concept of SVM is to find the best hyperplane in a high-dimensional space that can be used to separate the data into different classes. In a two-dimensional space, this hyperplane is a line, and in a three-dimensional space, it is a plane. The goal of SVM is to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the closest data points from each class. The data points that are closest to the hyperplane are called support vectors. SVM works by transforming the data into a higher-dimensional space, where a linear boundary can be used to separate the data. SVM is a powerful algorithm that can handle non-linearly separable data by using kernel functions. The kernel function maps the data into a higher-dimensional space, where a linear boundary can be used to separate the data. SVM is widely used in various fields, such as bioinformatics, text classification, and image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a88d1",
   "metadata": {},
   "source": [
    "2. What is the concept of a support vector?\n",
    "\n",
    "Ans:\n",
    "In support vector machines (SVM), a support vector is a data point that is closest to the hyperplane that separates the two classes in a binary classification problem. The hyperplane is determined by the SVM algorithm in such a way that it maximizes the margin between the two classes. The support vectors are the data points that lie closest to the hyperplane, and they play a crucial role in determining the location of the hyperplane and the classification accuracy of the SVM model. The decision boundary of an SVM model is determined by these support vectors and their distance from the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b1ce1",
   "metadata": {},
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "Ans:\n",
    "When using Support Vector Machines (SVMs), it is necessary to scale the inputs because SVMs are sensitive to the scale of the input features. SVMs attempt to maximize the margin between the separating hyperplane and the closest points of each class, and this margin is calculated based on the distance between the hyperplane and the closest points. If some input features have a much larger scale than others, they may dominate the calculation of the distance, resulting in the SVM ignoring other input features that may also be important for classification. Scaling the inputs helps to ensure that all input features are given equal consideration by the SVM. Common scaling techniques include normalization and standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd559bb",
   "metadata": {},
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?\n",
    "\n",
    "Ans:\n",
    "Yes, an SVM classifier can output a confidence score. The distance between the test instance and the decision boundary is a measure of the confidence of the classification. A larger distance indicates a higher confidence level for the classification. \n",
    "\n",
    "However, SVMs do not output a percentage chance or probability score directly as some other classifiers do. This is because SVMs are binary classifiers and only output a categorical label for each instance. To obtain a probability score, some modifications to the SVM model or post-processing techniques can be applied, such as Platt scaling or using a sigmoid function.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58395454",
   "metadata": {},
   "source": [
    "5. Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?\n",
    "\n",
    "Ans:\n",
    "When dealing with a large number of instances, it is often more computationally efficient to use the dual form of the SVM problem. This is because the primal form involves solving for the weight vector directly, which can be time-consuming and memory-intensive for large datasets. The dual form, on the other hand, involves computing the inner product between pairs of instances, which can be more efficient in high-dimensional spaces. However, the choice between primal and dual forms ultimately depends on the specific characteristics of the dataset and the resources available for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178c5ba",
   "metadata": {},
   "source": [
    "6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "Ans:\n",
    "If the SVM classifier using an RBF kernel appears to underfit the training set, it means that the decision boundary is too simple, and the model is not capturing the complexity of the data. In this case, we need to increase the model's complexity by either reducing the regularization parameter C or increasing the gamma parameter.\n",
    "\n",
    "The regularization parameter C determines the trade-off between finding the maximum margin hyperplane and ensuring that the instances are not misclassified. A small value of C makes the margins wider, and the model will tolerate more misclassifications, resulting in a more complex model. So, to increase the model's complexity, we should reduce the C value.\n",
    "\n",
    "The gamma parameter determines the range of influence of a single training instance. A low gamma means that the model is influenced by a larger area, while a high gamma means that the model is more sensitive to the training data. So, to increase the model's complexity, we should increase the gamma value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2c359",
   "metadata": {},
   "source": [
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "Ans:\n",
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "- H: This is the matrix of size (m+1) x (m+1) where m is the number of training instances. The matrix should be constructed using the kernel function and the training instances. Specifically, H_ij = y^(i) y^(j) x^(i) . x^(j) , where y^(i) is the target class of the i-th instance, x^(i) is the feature vector of the i-th instance, and the kernel function is used to compute the inner product of two feature vectors. The matrix should be positive semidefinite to ensure the QP solver can find a global minimum.\n",
    "\n",
    "- f: This is the vector of size (m+1) x 1 and should be set to all -1. This is because the objective function for the soft margin linear SVM classifier problem is 1/2 ||w||^2 - C * sum_i=1^m ξ_i, where ξ_i is the slack variable for the i-th instance, and C is the hyperparameter that controls the trade-off between the margin size and the number of margin violations. The QP solver minimizes this objective function, which is equivalent to minimizing -1/2 ||w||^2 + C * sum_i=1^m ξ_i.\n",
    "\n",
    "- A: This is the matrix of size m x (m+1) and should be set to the transpose of the matrix of training instances with an extra column of 1s added at the beginning. Specifically, A_ij = y^(i) x^(i)_j for i = 1, 2, ..., m and j = 1, 2, ..., n + 1, where n is the number of features. The matrix A encodes the constraints that the margin should be at least 1 for each training instance. The extra column of 1s corresponds to the bias term.\n",
    "\n",
    "- b: This is the vector of size m x 1 and should be set to all 1s. This encodes the constraints that the margin should be at least 1 for each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94997bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
