{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8246c0",
   "metadata": {},
   "source": [
    "1. Provide an example of the concepts of Prior, Posterior, and Likelihood.\n",
    "\n",
    "Ans:\n",
    "Consider a medical diagnosis problem where a doctor is trying to determine whether a patient has a certain disease based on their symptoms. Let D be the event that the patient has the disease, and S be the event that the patient exhibits certain symptoms.\n",
    "\n",
    "- Prior: The doctor has some prior belief about the prevalence of the disease in the population, represented by the prior probability P(D). For example, the doctor may know from past experience or from medical statistics that the disease affects 1% of the population, so the prior probability is P(D) = 0.01.\n",
    "\n",
    "- Likelihood: The doctor also has some knowledge about how likely each symptom is to occur given that the patient has the disease, represented by the likelihood function P(S|D). For example, the doctor may know from medical research that patients with the disease have a 90% chance of exhibiting symptom A and a 70% chance of exhibiting symptom B, so the likelihood function is P(S|D) = P(A and B|D) = P(A|D) * P(B|D) = 0.9 * 0.7 = 0.63.\n",
    "\n",
    "- Posterior: Based on the patient's symptoms, the doctor wants to update their belief about the probability of the patient having the disease, represented by the posterior probability P(D|S). Using Bayes' theorem, the doctor can compute the posterior probability as:\n",
    "\n",
    "  P(D|S) = P(S|D) * P(D) / P(S)\n",
    "\n",
    "  where P(S) is the marginal probability of observing the symptoms, which can be computed using the law of total probability:\n",
    "\n",
    "  P(S) = P(S|D) * P(D) + P(S|not D) * P(not D)\n",
    "\n",
    "  where not D is the complement event of D (i.e., the event that the patient does not have the disease). Assuming the doctor has some knowledge about the likelihood of the symptoms given that the patient does not have the disease (e.g., based on medical research or personal experience), they can compute P(S|not D) and P(not D) and substitute into the above equation to obtain P(S).\n",
    "\n",
    "  For example, suppose the patient exhibits symptoms A and B. Then, the posterior probability of the patient having the disease is:\n",
    "\n",
    "  P(D|A and B) = P(A and B|D) * P(D) / [P(A and B|D) * P(D) + P(A and B|not D) * P(not D)]\n",
    "\n",
    "  Using the values from above, this can be computed as:\n",
    "\n",
    "  P(D|A and B) = 0.63 * 0.01 / [0.63 * 0.01 + P(A and B|not D) * (1 - 0.01)]\n",
    "\n",
    "  If the doctor knows that the likelihood of observing symptoms A and B given that the patient does not have the disease is low (e.g., 0.01), then the posterior probability of the patient having the disease can be updated to:\n",
    "\n",
    "  P(D|A and B) = 0.63 * 0.01 / [0.63 * 0.01 + 0.01 * 0.99]\n",
    "\n",
    "  P(D|A and B) ≈ 0.39\n",
    "\n",
    "  which means the doctor's belief about the probability of the patient having the disease has increased from the prior probability of 0.01 to the posterior probability of 0.39, based on the patient's symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410d040",
   "metadata": {},
   "source": [
    "2. What role does Bayes&#39; theorem play in the concept learning principle?\n",
    "\n",
    "Ans:\n",
    "Bayes' theorem plays a crucial role in the concept learning principle by providing a framework for updating our beliefs or knowledge about a concept based on new evidence or data. The concept learning principle is based on the idea that individuals learn new concepts by adjusting their beliefs about the concept based on observations or evidence. Bayes' theorem provides a mathematical formula for this process of updating beliefs based on new information.\n",
    "\n",
    "In the context of concept learning, Bayes' theorem can be used to calculate the probability of a particular concept given some observed data or evidence. This is known as the posterior probability. The prior probability represents our initial belief or knowledge about the concept before we have observed any data. The likelihood function represents the probability of observing the data given the concept.\n",
    "\n",
    "By using Bayes' theorem, we can update our prior belief about the concept based on the observed data, and obtain a new posterior probability. This process of updating beliefs based on data is essential in concept learning, as it allows individuals to refine their understanding of a concept over time as they gather more evidence and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75d614",
   "metadata": {},
   "source": [
    "3. Offer an example of how the Nave Bayes classifier is used in real life.\n",
    "\n",
    "Ans:\n",
    "The Naive Bayes classifier is a popular algorithm used for classification tasks, particularly in natural language processing and text analysis. It is widely used in many real-life applications, such as email spam filtering, sentiment analysis, and document classification.\n",
    "\n",
    "One example of how the Naive Bayes classifier is used in real life is in email spam filtering. The classifier can be trained on a dataset of emails labeled as spam or non-spam, and then used to classify new incoming emails as either spam or non-spam. The classifier works by calculating the probability of an email being spam or non-spam based on the presence or absence of certain words or phrases in the email.\n",
    "\n",
    "For instance, if the word \"viagra\" appears frequently in spam emails, and rarely in legitimate emails, the Naive Bayes classifier will learn to associate the presence of this word with the probability that an email is spam. When a new email arrives, the classifier will calculate the probability of it being spam or non-spam based on the occurrence of such words and phrases in the email. It then assigns a label to the email based on the highest probability score.\n",
    "\n",
    "Another example of Naive Bayes in real life is in sentiment analysis, where it is used to classify text as positive, negative, or neutral based on the presence of certain words or phrases associated with different emotions. For example, the word \"happy\" is usually associated with positive sentiment, while the word \"hate\" is associated with negative sentiment. The classifier can be trained on a dataset of labeled text and used to classify new text based on the probability of it being positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00a1a5",
   "metadata": {},
   "source": [
    "4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about\n",
    "doing it?\n",
    "\n",
    "Ans:\n",
    "Yes, the Naive Bayes classifier can be used on continuous numeric data. One approach is to discretize the continuous data into a set of discrete values or intervals. This process is called binning. Once the data is discretized, the Naive Bayes classifier can be trained on the resulting discrete data.\n",
    "\n",
    "For example, suppose we have a dataset of customer purchase histories with two continuous features: the amount of money spent on the purchase and the number of items purchased. We can discretize these features into a set of bins, such as small, medium, and large, based on the ranges of values. Then, we can use the discretized data to train the Naive Bayes classifier to predict whether a customer is likely to purchase a specific product in the future.\n",
    "\n",
    "Another approach for using Naive Bayes with continuous data is to assume that the data follows a certain probability distribution, such as a normal distribution. In this case, we can estimate the parameters of the probability distribution from the training data and use them to calculate the likelihood of the data given a specific class. We can then use Bayes' theorem to compute the posterior probability of each class given the data and make a classification decision based on the highest posterior probability.\n",
    "\n",
    "Overall, the approach for using Naive Bayes with continuous data depends on the specific problem and the characteristics of the data. It is important to carefully choose the appropriate method to ensure accurate and reliable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acba773",
   "metadata": {},
   "source": [
    "5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they\n",
    "capable of resolving a wide range of issues?\n",
    "\n",
    "Ans:\n",
    "Bayesian Belief Networks (BBNs), also known as Bayesian Networks or graphical models, are probabilistic graphical models that represent the relationships between variables and their conditional dependencies. BBNs consist of a set of nodes representing random variables, and edges connecting these nodes to represent the dependencies between them. The nodes can represent either observable or hidden variables, and each node has a conditional probability table (CPT) that specifies the probability distribution of that node given its parents.\n",
    "\n",
    "BBNs work by propagating information through the network to compute the posterior probability distribution of the target variable given the evidence. This is achieved by applying Bayes' theorem and using the network structure to efficiently compute the joint probability distribution of all variables. BBNs can be used for a wide range of tasks, including classification, regression, clustering, anomaly detection, and decision making under uncertainty.\n",
    "\n",
    "BBNs have many applications in various fields, including healthcare, finance, engineering, environmental science, and artificial intelligence. For example, in healthcare, BBNs can be used for diagnosis, prognosis, and treatment planning. In finance, BBNs can be used for credit scoring, fraud detection, and portfolio optimization. In engineering, BBNs can be used for fault diagnosis, system design, and optimization.\n",
    "\n",
    "BBNs are capable of resolving a wide range of issues, but their effectiveness depends on the quality and completeness of the data and the accuracy of the model assumptions. BBNs can handle uncertainty and incomplete information, but they require a significant amount of data and domain expertise to build accurate models. In addition, the complexity of the model can increase exponentially with the number of variables, making it difficult to scale to large problems. However, with advances in machine learning and computational methods, BBNs have become more powerful and are increasingly used in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc50b4",
   "metadata": {},
   "source": [
    "6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the\n",
    "random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the\n",
    "variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98\n",
    "and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered,\n",
    "implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) =\n",
    "0.00001. What are the chances that an alarm would be triggered when an individual is actually an\n",
    "intruder?\n",
    "\n",
    "Ans:\n",
    "We need to find the probability of an alarm being triggered when an individual is actually an intruder, i.e., P(I = 1|A = 1).\n",
    "\n",
    "Using Bayes' theorem, we can write:\n",
    "\n",
    "P(I = 1|A = 1) = P(A = 1|I = 1) * P(I = 1) / P(A = 1)\n",
    "\n",
    "Here, P(A = 1) can be calculated using the law of total probability:\n",
    "\n",
    "P(A = 1) = P(A = 1|I = 1) * P(I = 1) + P(A = 1|I = 0) * P(I = 0)\n",
    "\n",
    "Substituting the given values, we get:\n",
    "\n",
    "P(A = 1) = (0.98 * 0.00001) + (0.001 * 0.99999) ≈ 0.0010098\n",
    "\n",
    "Now, we can substitute the calculated values in the first equation to get:\n",
    "\n",
    "P(I = 1|A = 1) = (0.98 * 0.00001) / 0.0010098 ≈ 0.0097\n",
    "\n",
    "Therefore, the chances that an alarm would be triggered when an individual is actually an intruder are approximately 0.0097 or 0.97%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59732b6",
   "metadata": {},
   "source": [
    "7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are\n",
    "not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of\n",
    "those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those\n",
    "actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were\n",
    "antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune\n",
    "(random variable D).\n",
    "\n",
    "Ans:\n",
    "We can use Bayes' theorem to calculate the likelihood that a person who tests positive is actually immune as follows:\n",
    "\n",
    "Let D be the event that a person is immune to the antibiotic, and let T be the event that the person tests positive in the antibiotic resistance test. Then we want to find P(D|T), the probability that the person is immune given that they tested positive.\n",
    "\n",
    "By Bayes' theorem, we have:\n",
    "\n",
    "P(D|T) = P(T|D) P(D) / P(T)\n",
    "\n",
    "where P(T|D) is the probability of testing positive given that the person is immune, P(D) is the prior probability of being immune, and P(T) is the probability of testing positive.\n",
    "\n",
    "We are given that:\n",
    "\n",
    "- P(T|D) = 0.95, the probability of testing positive given that the person is immune.\n",
    "- P(T|¬D) = 0.01, the probability of testing positive given that the person is not immune.\n",
    "- P(¬D) = 0.98, the complement of the prior probability of being immune (i.e., the probability of not being immune).\n",
    "- P(D) = 0.02, the prior probability of being immune.\n",
    "\n",
    "To calculate P(T), we can use the law of total probability:\n",
    "\n",
    "P(T) = P(T|D) P(D) + P(T|¬D) P(¬D)\n",
    "\n",
    "Substituting the values we have:\n",
    "\n",
    "P(T) = 0.95 x 0.02 + 0.01 x 0.98 = 0.0291\n",
    "\n",
    "Now we can calculate P(D|T):\n",
    "\n",
    "P(D|T) = P(T|D) P(D) / P(T) = 0.95 x 0.02 / 0.0291 ≈ 0.65\n",
    "\n",
    "Therefore, the likelihood that a person who tests positive is actually immune is about 65%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceaff4a",
   "metadata": {},
   "source": [
    "8.In order to prepare for the test, a student knows that there will be one question in the exam that\n",
    "is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and\n",
    "50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10\n",
    "type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "1. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "2. Given the student&#39;s solution, what is the likelihood that the problem was of form A?\n",
    "\n",
    "Ans:\n",
    "Let A, B, and C be the events of the exam problem being of type A, B, and C, respectively. Let E be the event that the student can solve the problem. Then we have:\n",
    "\n",
    "P(A) = 0.3, P(B) = 0.2, P(C) = 0.5 \n",
    "\n",
    "P(E|A) = 0.9, P(E|B) = 0.2, P(E|C) = 0.6\n",
    "\n",
    "1. We want to find P(E), the probability that the student can solve the exam problem. We can use the law of total probability:\n",
    "\n",
    "P(E) = P(E|A)P(A) + P(E|B)P(B) + P(E|C)P(C)\n",
    "\n",
    "     = (0.9)(0.3) + (0.2)(0.2) + (0.6)(0.5)\n",
    "     \n",
    "     = 0.48\n",
    "\n",
    "Therefore, the likelihood that the student can solve the exam problem is 0.48.\n",
    "\n",
    "2. Given that the student can solve the problem, we want to find the likelihood that the problem was of form A. We can use Bayes' theorem:\n",
    "\n",
    "P(A|E) = P(E|A)P(A) / P(E)\n",
    "\n",
    "       = (0.9)(0.3) / 0.48\n",
    "       \n",
    "       = 0.5625\n",
    "\n",
    "Therefore, the likelihood that the problem was of form A, given that the student can solve the problem, is 0.5625."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200b21",
   "metadata": {},
   "source": [
    "9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant\n",
    "influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into\n",
    "the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for\n",
    "simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If\n",
    "\n",
    "there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the\n",
    "camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "\n",
    "1. How many customers come into the bank on a daily basis (10 hours)?\n",
    "\n",
    "2. On a daily basis, how many fake photographs (photographs taken when there is no\n",
    "customer) and how many missed photographs (photographs taken when there is a customer) are\n",
    "there?\n",
    "\n",
    "3. Explain likelihood that there is a customer if there is a photograph?\n",
    "\n",
    "Ans:\n",
    "1. There are 10 hours in a day, which is 600 minutes. In each 5-minute bin, there is a 5% chance of a customer coming in, so on average, we expect 600/5 * 0.05 = 6 customers to come in per day.\n",
    "\n",
    "2. In a day, there are 600/5 = 120 time periods of 5 minutes each. For each of these time periods, there is a 5% chance of a customer coming in, so on average, we expect 120 * 0.05 = 6 customers to come in per day. If there is a customer, the CCTV will detect them with a 99% probability, so on average, we expect 6 * 0.99 = 5.94 photographs of customers per day. If there is no customer, the CCTV can take a false photograph with a 10% chance, so on average, we expect 120 - 6 = 114 time periods with no customers per day, and 114 * 0.1 = 11.4 false photographs per day. Therefore, on average, we expect 5.94 missed photographs and 11.4 fake photographs per day.\n",
    "\n",
    "3. If there is a photograph, there are two possibilities: either there is a customer, or there isn't. Let C be the event that there is a customer, and P be the event that there is a photograph. We want to find the likelihood that there is a customer given that there is a photograph, i.e., P(C|P). By Bayes' theorem, we have:\n",
    "\n",
    "P(C|P) = P(P|C) * P(C) / P(P)\n",
    "\n",
    "where P(P|C) is the probability of taking a photograph given that there is a customer, P(C) is the prior probability of there being a customer, and P(P) is the probability of taking a photograph. We know that P(P|C) = 0.99, P(C) = 0.05, and P(P) is the sum of the probabilities of taking a photograph given that there is a customer and taking a photograph given that there isn't:\n",
    "\n",
    "P(P) = P(P|C) * P(C) + P(P|~C) * P(~C)\n",
    "\n",
    "where ~C is the complement of C, i.e., the event that there isn't a customer. We know that P(P|~C) = 0.1, and P(~C) = 1 - P(C) = 0.95. Therefore,\n",
    "\n",
    "P(P) = 0.99 * 0.05 + 0.1 * 0.95 = 0.1045\n",
    "\n",
    "Plugging all these values into Bayes' theorem, we get:\n",
    "\n",
    "P(C|P) = 0.99 * 0.05 / 0.1045 = 0.474\n",
    "\n",
    "So the likelihood that there is a customer given that there is a photograph is 0.474."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32f09d",
   "metadata": {},
   "source": [
    "10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief\n",
    "network to represent the conditional independence assumptions of the Nave Bayes classifier for the\n",
    "match winning prediction problem in Section 6.4.4.\n",
    "\n",
    "Ans:\n",
    "To create the conditional probability table associated with the node Won Toss in the Bayesian Belief network, we need to define the conditional probabilities of the variable Won Toss given the class variable Match Result.\n",
    "\n",
    "Assuming that the outcome of the match can either be a win or a loss, the conditional probability table would look like:\n",
    "\n",
    "| Match Result | Won Toss = Yes | Won Toss = No |\n",
    "|--------------|----------------|---------------|\n",
    "| Win          | p(Y=Yes&#124;X=Win)        | p(Y=No&#124;X=Win)       |\n",
    "| Loss         | p(Y=Yes&#124;X=Loss)       | p(Y=No&#124;X=Loss)      |\n",
    "\n",
    "Here, X represents the set of all predictor variables in the model and Y represents the target variable. We can estimate these conditional probabilities from the training data using the maximum likelihood estimation method.\n",
    "\n",
    "For example, if the training data consists of 100 matches, and we observe that the team that won the toss also won the match in 70 of these matches, and lost in 30, and the team that lost the toss won the match in 40 matches and lost in 60, then the conditional probability table would be:\n",
    "\n",
    "| Match Result | Won Toss = Yes | Won Toss = No |\n",
    "|--------------|----------------|---------------|\n",
    "| Win          | 0.7            | 0.4           |\n",
    "| Loss         | 0.3            | 0.6           |\n",
    "\n",
    "We can then use this table to calculate the posterior probabilities of the variable Won Toss given the observed values of the predictor variables.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4805f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
