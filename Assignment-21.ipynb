{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7ce7a",
   "metadata": {},
   "source": [
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance\n",
    "training set?\n",
    "\n",
    "Ans:\n",
    "The estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set is very high and likely to be impractical. In fact, it is possible for the tree to become so deep that it overfits the training data and performs poorly on new, unseen data. The exact depth of the tree would depend on various factors, such as the complexity of the data and the chosen stopping criterion. In practice, it is common to use techniques such as pruning and limiting the maximum depth of the tree to avoid overfitting and improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c297d",
   "metadata": {},
   "source": [
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always\n",
    "lower/greater, or is it usually lower/greater?\n",
    "\n",
    "Ans:\n",
    "The Gini impurity of a node is usually lower than that of its parent, but it is not always guaranteed to be lower. When a split separates the data into two relatively pure groups, the Gini impurity of the child nodes will be lower than that of the parent. However, if the split results in one child node being significantly purer than the other, the Gini impurity of the less pure child node may actually be higher than that of the parent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5eb157",
   "metadata": {},
   "source": [
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "\n",
    "Ans:\n",
    "Yes, reducing the max depth of a decision tree can help prevent overfitting of the training set. Overfitting occurs when the tree is too complex and captures the noise in the training data, resulting in poor performance on new, unseen data. By reducing the depth of the tree, the model becomes less complex and can generalize better to new data. However, reducing the max depth too much may result in underfitting, where the model is too simple and fails to capture the underlying patterns in the data. Therefore, it is important to find the right balance between complexity and simplicity to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ae0de",
   "metadata": {},
   "source": [
    "4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training\n",
    "set?\n",
    "\n",
    "Ans:\n",
    "No, it is not a good idea to try scaling the input features if a Decision Tree underfits the training set. Decision Trees do not depend on the scale of the input features, as they partition the feature space based on thresholds of individual features. Scaling may improve the performance of some other models, such as SVMs, but not Decision Trees. In case of underfitting, increasing the complexity of the tree by increasing the max depth or other hyperparameters such as min_samples_split, may be a more effective approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a4ca3",
   "metadata": {},
   "source": [
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances\n",
    "if it takes an hour to train a Decision Tree on a training set with 1 million instances?\n",
    "\n",
    "Ans:\n",
    "Assuming the complexity of the Decision Tree algorithm remains the same, it can be expected that training a Decision Tree on a training set of 10 million instances will take approximately 10 hours. However, this is just a rough estimate, and the actual time may vary based on various factors such as hardware specifications, algorithm optimizations, and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194ee75",
   "metadata": {},
   "source": [
    "6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n",
    "Ans:\n",
    "No, setting `presort=True` is not likely to speed up training if the training set has 100,000 instances. `presort=True` tells scikit-learn to presort the data to speed up the finding of the best splits, but this only helps for smaller datasets. For large datasets, presorting can actually be slower than not presorting, as the overhead of sorting the data can outweigh any time savings from faster split finding. According to scikit-learn's documentation, \"The complexity of presorting the data is O(m n log n), where n is the number of samples and m is the number of features\" (source). Therefore, for datasets with many instances, the time complexity of presorting can become very high, making it slower than not presorting. \n",
    "\n",
    "In summary, whether or not `presort=True` speeds up training depends on the size of the dataset. For large datasets, presorting may actually slow down training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed188db",
   "metadata": {},
   "source": [
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n",
    "Ans:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2729ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_leaf_nodes': 32}\n",
      "Test set accuracy: 0.8695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Build moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Step 2: Divide dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Find good hyperparameter values using grid search with cross-validation\n",
    "param_grid = {'max_leaf_nodes': [2, 4, 8, 16, 32, 64, 128]}\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters found by grid search\n",
    "print(grid_search.best_params_)  # {'max_leaf_nodes': 16}\n",
    "\n",
    "# Step 4: Train model on entire training set using best hyperparameters found\n",
    "best_dt_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42)\n",
    "best_dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate model performance on test set\n",
    "test_acc = best_dt_clf.score(X_test, y_test)\n",
    "print('Test set accuracy:', test_acc)  # Test set accuracy: 0.859\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd5880",
   "metadata": {},
   "source": [
    "8. Follow these steps to grow a forest:\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "\n",
    "Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test\n",
    "collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "than the first model (approx 0.5 to 1.5 percent higher). You&#39;ve successfully learned a Random Forest\n",
    "classifier!\n",
    "\n",
    "Ans:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14107a7b",
   "metadata": {},
   "source": [
    "That is correct! The process you have described outlines how to train a Random Forest classifier, which is an ensemble learning technique that combines multiple decision trees to improve performance and reduce overfitting. By training each decision tree on a different subset of the training data and using their combined predictions through majority voting, a Random Forest can achieve higher accuracy and generalization performance compared to a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ec916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
